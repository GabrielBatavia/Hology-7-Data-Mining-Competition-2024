{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:14:17.522072Z",
     "start_time": "2024-10-21T11:14:09.630924Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "train_img_folder = '../Penyisihan_Hology_DataMining/train'  # Update with the path to your train folder\n",
    "csv_file = '../Penyisihan_Hology_DataMining/train.csv'  # Update with the path to your train.csv\n",
    "\n",
    "# Image preprocessing constants\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "# Load train.csv\n",
    "train_df = pd.read_csv(csv_file)\n",
    "\n",
    "# Helper function to load and resize images\n",
    "def load_images_from_folder(folder, img_ids, img_extensions=['.jpg', '.JPG', '.png']):\n",
    "    images = []\n",
    "    valid_img_ids = []  # To store ids of images with correct shape\n",
    "    for img_id in img_ids:\n",
    "        img_path = None\n",
    "        for ext in img_extensions:\n",
    "            potential_path = os.path.join(folder, f\"{img_id}{ext}\")\n",
    "            if os.path.exists(potential_path):\n",
    "                img_path = potential_path\n",
    "                break\n",
    "\n",
    "        if img_path:\n",
    "            try:\n",
    "                # Load and resize the image\n",
    "                img = load_img(img_path, target_size=IMG_SIZE)\n",
    "                img_array = img_to_array(img) / 255.0  # Normalize\n",
    "                images.append(img_array)\n",
    "                valid_img_ids.append(img_id)  # Store valid image ids\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process image {img_id}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Image {img_id} not found.\")\n",
    "    \n",
    "    return np.array(images), valid_img_ids\n",
    "\n",
    "# Load images and check shapes\n",
    "images, valid_img_ids = load_images_from_folder(train_img_folder, train_df['id'].values)\n",
    "\n",
    "# Filter the labels based on valid image ids\n",
    "train_df_filtered = train_df[train_df['id'].isin(valid_img_ids)]\n",
    "jenis_labels_filtered = train_df_filtered['jenis'].values\n",
    "warna_labels_filtered = train_df_filtered['warna'].values\n",
    "\n",
    "# Convert the filtered labels into categorical format\n",
    "jenis_labels_cat = to_categorical(jenis_labels_filtered, num_classes=2)\n",
    "warna_labels_cat = to_categorical(warna_labels_filtered, num_classes=5)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f'Bentuk X_train: {images.shape}')\n",
    "print(f'Bentuk jenis_labels_cat: {jenis_labels_cat.shape}')\n",
    "print(f'Bentuk warna_labels_cat: {warna_labels_cat.shape}')\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train_jenis, y_val_jenis, y_train_warna, y_val_warna = train_test_split(\n",
    "    images, jenis_labels_cat, warna_labels_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Model architecture: Multi-output CNN\n",
    "input_layer = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "# Convolutional layers with Batch Normalization and Dropout\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu')(pool2)\n",
    "conv3 = BatchNormalization()(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "conv4 = Conv2D(256, (3, 3), activation='relu')(pool3)\n",
    "conv4 = BatchNormalization()(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "conv5 = Conv2D(512, (3, 3), activation='relu')(pool4)\n",
    "conv5 = BatchNormalization()(conv5)\n",
    "pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "\n",
    "# Flatten layer\n",
    "flat = Flatten()(pool5)\n",
    "dropout = Dropout(0.5)(flat)\n",
    "\n",
    "# Output for 'jenis' (T-shirt or Hoodie)\n",
    "jenis_output = Dense(2, activation='softmax', name='jenis_output')(dropout)\n",
    "\n",
    "# Output for 'warna' (Red, Yellow, Blue, Black, White)\n",
    "warna_output = Dense(5, activation='softmax', name='warna_output')(dropout)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=input_layer, outputs=[jenis_output, warna_output])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'jenis_output': 'categorical_crossentropy', 'warna_output': 'categorical_crossentropy'}, \n",
    "              metrics={'jenis_output': 'accuracy', 'warna_output': 'accuracy'})\n",
    "\n",
    "# Learning Rate Reduction\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "\n",
    "# Prepare labels as a dictionary\n",
    "y_train_dict = {'jenis_output': y_train_jenis, 'warna_output': y_train_warna}\n",
    "\n",
    "# Helper function for data generator to handle multi-output\n",
    "def generator_with_multiple_outputs(image_generator, X, y_dict):\n",
    "    gen = image_generator.flow(X, y_dict, batch_size=32)\n",
    "    while True:\n",
    "        X_batch, y_batch = gen.next()\n",
    "        yield X_batch, y_batch  # y_batch is already a dictionary\n",
    "\n",
    "# Use the generator to fit the model\n",
    "train_gen = generator_with_multiple_outputs(datagen, X_train, y_train_dict)\n",
    "\n",
    "# Calculate steps_per_epoch\n",
    "steps_per_epoch = len(X_train) // 32\n",
    "\n",
    "# Validation data\n",
    "val_data = (X_val, {'jenis_output': y_val_jenis, 'warna_output': y_val_warna})\n",
    "\n",
    "# Train the model using the generator\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=val_data,\n",
    "                    epochs=20, callbacks=[reduce_lr])\n",
    "\n",
    "# Save the model\n",
    "model.save('./improved_model/multilabel_model.h5')\n",
    "\n",
    "# Plotting training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['jenis_output_accuracy'], label='Jenis Accuracy')\n",
    "plt.plot(history.history['warna_output_accuracy'], label='Warna Accuracy')\n",
    "plt.plot(history.history['val_jenis_output_accuracy'], label='Val Jenis Accuracy')\n",
    "plt.plot(history.history['val_warna_output_accuracy'], label='Val Warna Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['jenis_output_loss'], label='Jenis Loss')\n",
    "plt.plot(history.history['warna_output_loss'], label='Warna Loss')\n",
    "plt.plot(history.history['val_jenis_output_loss'], label='Val Jenis Loss')\n",
    "plt.plot(history.history['val_warna_output_loss'], label='Val Warna Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bentuk X_train: (777, 128, 128, 3)\n",
      "Bentuk jenis_labels_cat: (777, 2)\n",
      "Bentuk warna_labels_cat: (777, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (621, 128, 128, 3), y.shape = ()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 148\u001B[0m\n\u001B[0;32m    145\u001B[0m val_data \u001B[38;5;241m=\u001B[39m (X_val, {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjenis_output\u001B[39m\u001B[38;5;124m'\u001B[39m: y_val_jenis, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwarna_output\u001B[39m\u001B[38;5;124m'\u001B[39m: y_val_warna})\n\u001B[0;32m    147\u001B[0m \u001B[38;5;66;03m# Train the model using the generator\u001B[39;00m\n\u001B[1;32m--> 148\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_gen\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m                    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mreduce_lr\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# Save the model\u001B[39;00m\n\u001B[0;32m    154\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./improved_model/multilabel_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensor-env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Cell \u001B[1;32mIn[1], line 133\u001B[0m, in \u001B[0;36mgenerator_with_multiple_outputs\u001B[1;34m(image_generator, X, y_dict)\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerator_with_multiple_outputs\u001B[39m(image_generator, X, y_dict):\n\u001B[1;32m--> 133\u001B[0m     gen \u001B[38;5;241m=\u001B[39m \u001B[43mimage_generator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    135\u001B[0m         X_batch, y_batch \u001B[38;5;241m=\u001B[39m gen\u001B[38;5;241m.\u001B[39mnext()\n",
      "\u001B[1;31mValueError\u001B[0m: `x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (621, 128, 128, 3), y.shape = ()"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T09:38:15.169298Z",
     "start_time": "2024-10-21T09:38:15.154299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Bentuk X_train: {images.shape}')\n",
    "print(f'Bentuk jenis_labels_cat: {jenis_labels_cat.shape}')\n",
    "print(f'Bentuk warna_labels_cat: {warna_labels_cat.shape}')\n"
   ],
   "id": "bd243d3a9d5e1330",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bentuk X_train: (777, 128, 128, 3)\n",
      "Bentuk jenis_labels_cat: (777, 2)\n",
      "Bentuk warna_labels_cat: (777, 5)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a94eab07c7ab0f3a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
